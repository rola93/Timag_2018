<html><head><meta content="text/html; charset=UTF-8" http-equiv="content-type"><style type="text/css">ol{margin:0;padding:0}table td,table th{padding:0}.c8{border-right-style:solid;padding:5pt 5pt 5pt 5pt;border-bottom-color:#000000;border-top-width:1pt;border-right-width:1pt;border-left-color:#000000;vertical-align:top;border-right-color:#000000;border-left-width:1pt;border-top-style:solid;border-left-style:solid;border-bottom-width:1pt;width:150.5pt;border-top-color:#000000;border-bottom-style:solid}.c9{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:20pt;font-family:"Arial";font-style:normal}.c7{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:16pt;font-family:"Arial";font-style:normal}.c31{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:10pt;font-family:"Arial";font-style:normal}.c14{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:right;margin-right:19.2pt}.c22{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:9pt;font-family:"Arial";font-style:normal}.c27{color:#666666;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:15pt;font-family:"Arial";font-style:normal}.c28{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:26pt;font-family:"Arial";font-style:normal}.c1{margin-left:18pt;padding-top:3pt;padding-bottom:0pt;line-height:1.0;orphans:2;widows:2;text-align:left}.c35{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:14pt;font-family:"Arial";font-style:normal}.c0{color:#000000;font-weight:400;text-decoration:none;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c6{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:center}.c4{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:left}.c2{padding-top:0pt;padding-bottom:0pt;line-height:1.15;orphans:2;widows:2;text-align:justify}.c39{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c34{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:right}.c19{padding-top:0pt;padding-bottom:16pt;line-height:1.15;page-break-after:avoid;text-align:center}.c24{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:left}.c30{padding-top:20pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:justify}.c33{padding-top:0pt;padding-bottom:3pt;line-height:1.15;page-break-after:avoid;text-align:center}.c16{padding-top:18pt;padding-bottom:6pt;line-height:1.15;page-break-after:avoid;text-align:justify}.c17{font-weight:400;vertical-align:baseline;font-size:11pt;font-family:"Arial";font-style:normal}.c38{border-spacing:0;border-collapse:collapse;margin-right:auto}.c15{padding-top:0pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c23{padding-top:10pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c12{text-decoration-skip-ink:none;-webkit-text-decoration-skip:none;color:#1155cc;text-decoration:underline}.c37{padding-top:4pt;padding-bottom:0pt;line-height:1.0;text-align:left}.c36{padding-top:10pt;padding-bottom:4pt;line-height:1.0;text-align:left}.c25{background-color:#ffffff;max-width:451.4pt;padding:72pt 72pt 72pt 72pt}.c40{width:33%;height:1px}.c21{orphans:2;widows:2}.c10{color:inherit;text-decoration:inherit}.c3{height:11pt}.c5{height:0pt}.c13{font-style:italic}.c18{font-weight:700}.c11{margin-left:36pt}.c29{margin-left:-49.6pt}.c20{font-size:20pt}.c26{font-size:9pt}.c32{height:20pt}.title{padding-top:0pt;color:#000000;font-size:26pt;padding-bottom:3pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}.subtitle{padding-top:0pt;color:#666666;font-size:15pt;padding-bottom:16pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}li{color:#000000;font-size:11pt;font-family:"Arial"}p{margin:0;color:#000000;font-size:11pt;font-family:"Arial"}h1{padding-top:20pt;color:#000000;font-size:20pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h2{padding-top:18pt;color:#000000;font-size:16pt;padding-bottom:6pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h3{padding-top:16pt;color:#434343;font-size:14pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h4{padding-top:14pt;color:#666666;font-size:12pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h5{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;orphans:2;widows:2;text-align:left}h6{padding-top:12pt;color:#666666;font-size:11pt;padding-bottom:4pt;font-family:"Arial";line-height:1.15;page-break-after:avoid;font-style:italic;orphans:2;widows:2;text-align:left}</style></head><body class="c25"><p class="c33 title" id="h.ygkld9xfnk3j"><span class="c28">Procesamiento de imágenes laparoscópicas</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c19 subtitle" id="h.qrefes9rsb01"><span class="c27">Tratamiento de Imágenes 2018 - Rodrigo Laguna</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">El
 presente trabajo es una primera aproximación al procesamiento de 
imágenes laparoscópicas a partir de videos provistos por el Hospital de 
Clínicas, en el marco del proyecto final del curso de Tratamiento de 
Imágenes, edición 2018. Pretende abarcar dos problemas: por un lado la 
detección y corrección de especularidades, y por otro, la segmentación 
de instrumentos quirúrgicos. </span></p><h1 class="c34" id="h.msyzc1cwg7ko"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 151.00px; height: 79.00px;">
<a href="https://github.com/rola93/Timag_2018" target="_blank">
<img alt="" src="Informeconsolidado_files/image43.png" style="width: 151.00px; height: 79.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title="">
</a>
</span></h1><p class="c14"><span class="c12 c26">

<a class="c10" href="https://github.com/rola93/Timag_2018" target="_blank">Código disponible!</a></span></p><p class="c4"><span class="c20">Contenidos</span></p><p class="c4 c3"><span class="c0"></span></p>

<p class="c21 c37"><span class="c12"><a class="c10" href="#h.ep5s587uu79b">Detección y corrección de especularidades</a></span></p><p class="c1"><span class="c12"><a class="c10" href="#h.ytjs35dtnmsy">Detección mediante umbralización</a></span></p><p class="c1"><span class="c12"><a class="c10" href="#h.lrvq06maq52n">Detección mediante aprendizaje</a></span></p><p class="c21 c23"><span class="c12"><a class="c10" href="#h.c9sjkd3l7zys">Segmentación de instrumentos</a></span></p><p class="c1"><span class="c12"><a class="c10" href="#h.xfg4imgbzw1d">Selección de atributos</a></span></p><p class="c1"><span class="c12"><a class="c10" href="#h.pd1dna3ci7ma">Entrenamiento</a></span></p><p class="c1"><span class="c12"><a class="c10" href="#h.y5k2n7ewozzm">Testing</a></span></p><p class="c1"><span class="c12"><a class="c10" href="#h.r3ts95vpgma4">Análisis cualitativo</a></span></p><p class="c21 c36"><span class="c12"><a class="c10" href="#h.u93bpobo0wn">Conclusiones y trabajo a futuro</a></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><h1 class="c39" id="h.ep5s587uu79b"><span class="c9">Detección y corrección de especularidades</span></h1><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Las
 reflexiones especulares afectan fuertemente la apariencia de las 
imágenes y generalmente dificultan los algoritmos de visión por 
computadora aplicados a ellas. Los reflejos creados por reflexiones 
especulares son un gran obstáculo para tareas de más alto nivel, como la
 segmentación automática de estas imágenes. </span></p><p class="c4 c3"><span class="c0"></span></p><h2 class="c24" id="h.ytjs35dtnmsy"><span class="c7">Detección mediante umbralización</span></h2><p class="c4 c3"><span class="c0"></span></p><p class="c2">

<span>En esta sección se analizan las pruebas hechas para el problema de </span>

<span class="c13">detección y corrección de especularidades</span><span>, basado en el trabajo de </span><span class="c12"><a class="c10" href="https://pdfs.semanticscholar.org/c4cf/a0783866a30e1c8df3cf7b6780196b05ef48.pdf" target="_blank">G. Zimmerman-Moreno and H. Greenspan</a></span><sup><a href="#ftnt1" id="ftnt_ref1">[1]</a></sup><span class="c0">.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">A
 continuación se muestra paso a paso lo que proponen sus autores y las 
modificaciones hechas en el presente trabajo, con su correspondiente 
argumento. Debido a que no existen datos etiquetados para esta etapa, la
 comparación entre enfoques y análisis de resultados es puramente 
cualitativa. En el trabajo de referencia se realiza una evaluación final
 en función de la opinión de un Médico respecto a la calidad de las 
imágenes obtenidas.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Para
 este trabajo, se dispone de cuatro videos tomados sin supervisión por 
parte del Hospital de Clínicas. Únicamente se utilizó uno para ajustar 
los valores, y otro para validarlos, ya que es una fase exploratoria del
 problema. Se tomaron 10 frames, salteando otros 10 frames entre cada 
uno de ellos. Como referencia en este informe se utiliza el primero de 
ellos, que es el siguiente:</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 587.00px; height: 482.00px;"><img alt="" src="Informeconsolidado_files/image28.png" style="width: 587.00px; height: 482.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3 c11"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">A partir de cada frame, se obtiene una imagen S y otra I, siguiendo la siguiente fórmula, propuesta por los autores:</span></p><p class="c4 c3 c11"><span class="c0"></span></p><p class="c6 c11"><img src="Informeconsolidado_files/image1.png"><span>&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; </span><img src="Informeconsolidado_files/image2.png"></p><p class="c4 c3 c11"><span class="c0"></span></p><p class="c2"><span>La I la definen como la </span><span class="c13">intensidad</span><span class="c0">,
 es el promedio de los canales. Sobre la imagen resultante se umbraliza,
 definiendo como especularidades aquellos píxeles con mayor valor que 
cierto umbral. De esta manera se mantienen los píxeles que están en 
promedio cercanos al blanco.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>La S la definen como la </span><span class="c13">saturación</span><span class="c0">.
 Es un poco más difícil de interpretar. En este caso se toman los que 
están debajo del umbral. Tiene su mínimo (0) cuando el mínimo de los 
tres canales es igual al promedio, o sea, cuando los tres canales valen 
lo mismo.</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 587.00px; height: 482.00px;"><img alt="" src="Informeconsolidado_files/image51.png" style="width: 587.00px; height: 482.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3 c11"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 587.00px; height: 482.00px;"><img alt="" src="Informeconsolidado_files/image42.png" style="width: 587.00px; height: 482.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3 c11"><span class="c0"></span></p><p class="c2"><span class="c0">Para
 ser considerado candidato a especularidad, un píxel debe cumplir ambos 
criterios. Aplicando los umbrales a ambas imágenes, cuyos valores se 
ajustaron respecto de los reportados en el trabajo de referencia, se 
detectan las siguientes regiones:</span></p><p class="c4 c3 c11"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image11.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Luego,
 sobre la imagen I se toma el módulo del gradiente, y se aplica un 
umbral. Esto en el trabajo original tiene mucho sentido ya que en esas 
imágenes particulares, de cervicales, tienen especularidades muy 
pequeñas. Al imponer esta condición sobre el gradiente, logran refinar 
los candidatos que surgen del paso anterior. En este trabajo se lo va a 
usar para ampliar el conjunto de candidatos ya que algunas 
especulaciones puntuales muy pequeñas no las ha detectado bien con los 
primeros dos criterios. El módulo del gradiente es el siguiente:</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 587.00px; height: 482.00px;"><img alt="" src="Informeconsolidado_files/image18.png" style="width: 587.00px; height: 482.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4"><span class="c0">Y cuando se toma el umbral, se detectan las siguientes especularidades:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image50.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c3 c4"><span class="c0"></span></p><p class="c2"><span class="c0">Notar
 que algunos de estos píxeles ya fueron detectados con el criterio 
presentado anteriormente (umbralizando sobre I y S), que también son 
detectados mediante el gradiente. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Se
 toma los tres criterios, de manera que un punto para ser considerado 
especularidad, debe ser o por el criterio de umbral sobre I y S, o por 
el criterio del módulo del gradiente. Esto es ligeramente distinto ya 
que el trabajo de referencia usaba el criterio del gradiente para 
refinar los candidatos, mientras que acá se lo utiliza para ampliar el 
conjunto de candidatos.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Se
 muestra a continuación los píxeles candidatos según los estos 
criterios. Los píxeles detectados como especularidades son los 
siguientes:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image39.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">En
 este punto, se tienen píxeles candidatos a especularidades, pero no 
todos ellos lo son. En el trabajo de referencia, se toman estos píxeles y
 se los modela en el espacio de colores HSV, considerando únicamente las
 dimensiones SV.</span></p><p class="c4 c3"><span class="c0"></span></p><hr style="page-break-before:always;display:none;"><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span class="c0">El cono HSV es el siguiente:</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 320.00px; height: 282.00px;"><img alt="" src="Informeconsolidado_files/image9.png" style="width: 320.00px; height: 282.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c2"><span class="c0">En
 el trabajo de referencia, plantean que en el espacio SV, las 
especularidades van a tener valores bajos de saturación S, y altos de 
value V. Con esto, identifican 4 conjuntos de píxeles: los de las 
especularidades, y otros tres que corresponden a distintos tipos de 
tejido e instrumentos. Los autores modelan estos puntos como una mezcla 
de 4 gaussianas, y toman como píxeles de especulaciones aquellos de la 
gausiana con V alto y S bajo. Para ver cómo se distribuyen estos píxeles
 en este caso, en primer lugar se grafican estos puntos:</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 333.33px;"><img alt="" src="Informeconsolidado_files/image25.png" style="width: 602.00px; height: 333.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">En
 esta imagen se muestran 152.922 puntos. Como son muchos, la gran 
mayoría se superponen. Sin embargo se puede ver más densamente poblados 
los píxeles de Value alto y Saturation baja. A continuación se muestra 
un histograma de estos valores.</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 361.33px;"><img alt="" src="Informeconsolidado_files/image15.png" style="width: 602.00px; height: 361.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Puede
 notarse que se repite lo mismo, aunque se nota una concentración mucho 
mayor en los valores muy altos de value. Sin embargo no se notan grandes
 nubes de puntos.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>Con
 estas observaciones, y con la idea de refinar los valores, se toma un 
enfoque de detección de outliers. Para esto, de lo disponible en </span><span class="c12"><a class="c10" href="http://scikit-learn.org/stable/auto_examples/covariance/plot_outlier_detection.html" target="_blank">sklearn</a></span><span>&nbsp;se probó con </span><span class="c12"><a class="c10" href="http://scikit-learn.org/stable/modules/generated/sklearn.svm.OneClassSVM.html" target="_blank">OneClassSVM</a></span><span>, </span><span class="c12"><a class="c10" href="http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.LocalOutlierFactor.html" target="_blank">Local Outlier Factor </a></span><span>e </span><span class="c12"><a class="c10" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.IsolationForest.html" target="_blank">Isolation Forest</a></span><span>. El que arrojó mejores resultados fue este último. Notar que esto es completamente subjetivo ya que no existe </span><span class="c13">ground truth</span><span class="c0">&nbsp;para comparar. Con este paso, los puntos quedarían de la siguiente manera:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 333.33px;"><img alt="" src="Informeconsolidado_files/image36.png" style="width: 602.00px; height: 333.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">A
 continuación se muestra cómo impacta esta detección de outliers en la 
imagen. A la izquierda se ve la imagen original, y a la derecha la 
imagen con las especulaciones pintadas de azul y los outliers con verde.</span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 262.67px;"><img alt="" src="Informeconsolidado_files/image12.png" style="width: 602.00px; height: 262.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Si
 bien son muy pocos, parecen tener sentido. Un problema con este enfoque
 &nbsp;es que el porcentaje de outliers tiene que ser estimado a priori 
(en este caso fue arbitrariamente fijado en 5%). Con todos los pasos 
descritos hasta el momento se encuentran las siguientes especularidades:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 587.00px; height: 482.00px;"><img alt="" src="Informeconsolidado_files/image45.png" style="width: 587.00px; height: 482.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span>Llegado a este punto, se realiza el inpainting utilizando </span><span class="c12"><a class="c10" href="https://docs.opencv.org/ref/master/df/d3d/tutorial_py_inpainting.html" target="_blank">la implementación de OpenCV</a></span><span>.
 Esta librería provee dos implementaciones, se probaron ambas sin notar 
grandes diferencias. Finalmente se utilizó el algoritmo propuesto por 
Alexandru Telea</span><sup><a href="#ftnt2" id="ftnt_ref2">[2]</a></sup><span class="c0">.
 A grandes rasgos este algoritmo toma los píxeles del perímetro de la 
especularidad a rellenar, y los promedia ponderando según la distancia 
del borde al píxel que se está rellenando. Esto lo hace de forma 
iterativa, hasta rellenar toda la especularidad. El resultado es el 
siguiente:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 262.67px;"><img alt="" src="Informeconsolidado_files/image8.png" style="width: 602.00px; height: 262.67px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3 c29"><span class="c0"></span></p><p class="c2"><span class="c0">La
 parte con grasa de la imagen se resuelve muy bien, sin embargo, en las 
partes rojas quedan manchas violetas. Mirando de cerca una región en 
particular, la causa del error se hace evidente.</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 224.00px;"><img alt="" src="Informeconsolidado_files/image41.png" style="width: 602.00px; height: 224.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">En
 torno a las especulaciones queda un resplandor azulado que luego el 
inpainting propaga hacia la especularidad. Se puede corroborar mirando 
de cerca la imagen con la máscara de especularidades detectadas:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 421.33px;"><img alt="" src="Informeconsolidado_files/image35.png" style="width: 602.00px; height: 421.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0">&nbsp;Una
 solución rápida que se evaluó fue dilatar la máscara para que cubra el 
resplandor mencionado. Dilatando esta máscara antes de realizar el 
inpainting, se llega a la siguiente imagen:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 224.00px;"><img alt="" src="Informeconsolidado_files/image46.png" style="width: 602.00px; height: 224.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Si
 bien el azulado desaparece, la imagen resultante queda algo borrosa. 
Este problema se hace más notorio cuando la especularidad está sobre un 
instrumento. Por ejemplo, la tijera tiene especularidades en casi toda 
su superficie. Esto hace que, incluso sin agrandar la máscara, al 
realizar el inpainting, la tijera sea rellenada con colores del tejido. 
Se muestra a continuación un frame de ejemplo donde aparece una tijera, 
corregida con las dos versiones.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image30.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Usando
 la máscara dilatada el azulado desaparece casi por completo, pero 
también lo hace la tijera. Con esto, se opta por descartar la dilatación
 de la máscara, porque es preferible que la reconstrucción pierda 
calidad a que destruya la tijera.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Para
 finalizar, se utiliza la configuración descrita anteriormente para 
corregir imágenes de otro video, que hasta ahora no había usado, a los 
efectos de validar el enfoque.</span></p><hr style="page-break-before:always;display:none;"><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image20.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">El
 resultado no es muy bueno. La imagen se distorsiona bastante en algunas
 zonas, mientras que existen especularidades que se mantienen 
incambiadas. En este punto no se invirtió tiempo en ver cuál paso es el 
que introduce el error, aunque posiblemente sean varios. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>Para
 llegar a este punto, se hicieron varias pruebas intermedias sobre si 
incluir o no el gradiente, detectar los outliers usando </span><span>distintos algoritmos</span><span class="c0">,
 y varias combinaciones de parámetros. Lo mejor a lo que se llegó fue lo
 que expuesto en el presente informe, y todas las variantes probadas 
tienen el inconveniente de el resplandor azul, que en los trabajos 
previos considerados no se menciona.</span></p><p class="c2 c3"><span class="c0"></span></p><h2 class="c24" id="h.lrvq06maq52n"><span class="c7">Detección mediante aprendizaje</span></h2><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Llegado
 este punto, se opta por dejar de lado este enfoque momentáneamente y 
pasar a segmentar píxeles clasificándolos en especularidad/no 
especularidad. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Con
 este objetivo, se generó manualmente una máscara para las 
especularidades, considerando las partes azuladas como especularidades. 
Durante el proceso de anotación, que tomó unas cuantas horas, se 
observaron dos cosas: como el resplandor azulado es en degradé, es 
difícil definir hasta qué punto es resplandor y en cuál no. Por otro 
lado, en algunos puntos claros aislados es difícil determinar si se 
trata de especularidades o es el color del tejido. Como además las 
especularidades en algunos casos son muy pequeñas, esto lleva a que 
quizá no todas las especularidades fueron marcadas correctamente, y 
quizá fueron marcadas algunas partes que en realidad no lo eran. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>Con esta máscara como </span><span class="c13">ground truth</span><span class="c0">,
 el primer paso fue hacer el inpainting, ya que en el mejor de los casos
 un clasificador aprenderá esa máscara. El resultado fue el siguiente:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image17.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Si
 bien esta imagen no está perfecta, es mejor que la que encontrada 
umbralizando. Con esta motivación se exploran clasificadores para esta 
tarea. Cómo features se toman los que ya se habían usado: el color en 
HSV y RGB, la I y S definidas en el trabajo de referencia, y el 
gradiente sobre I, ya que con estos valores en definitiva se habían 
logrado los resultados previos.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">De los píxeles de la imagen, se toman el 20% para la validación final y el resto para entrenamiento.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>Se
 probaron varios clasificadores, y para cada uno de ellos varios 
hiperparametros. En cada uno de ellos se intentó maximizar el promedio 
de la medida F1 durante una validación cruzada con 10 folds, tomando 
como clase positiva a las especularidades. Se tomó como referencia la 
medida F1 porque es un buen compromiso entre </span><span class="c13">presicion</span><span>&nbsp;y </span><span class="c13">recall</span><span class="c0">, y no hay una clara preferencia por ninguna de estas.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">El
 mejor resultado obtenido fue con KNN, con F1 = 0.712 (+/-0.020), por lo
 que se prosigue a realizar la evaluación sobre los píxeles que había 
guardado para testear. El resultado fue el siguiente:</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c18">&nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp;Precision &nbsp; &nbsp;Recall &nbsp;F1-score 
&nbsp; Support</span><span><br><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c18">no espec.</span><span>&nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;0.99 &nbsp; &nbsp; &nbsp; &nbsp; 1.00 &nbsp;
 &nbsp; &nbsp; 0.99 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;80406<br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c18">espec. &nbsp; &nbsp; &nbsp;</span><span>&nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp;0.81 &nbsp; &nbsp; &nbsp; &nbsp; 0.62 &nbsp;
 &nbsp; &nbsp; 0.70 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;2538<br><br> &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;</span><span class="c18">avg / total &nbsp; &nbsp; &nbsp; &nbsp; </span><span class="c0">0.98 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.98 &nbsp; &nbsp; &nbsp; 0.98 &nbsp; &nbsp; &nbsp; &nbsp; 82944<br><br>Con
 estos resultados, se toma el clasificador entrenado y se crea la 
máscara correspondiente a la imagen. Notar que para este resultado, el 
80% de los píxeles fueron parte del entrenamiento.</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image49.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">La
 máscara aprendida, pese a que el 80% de los píxeles estuvieron 
presentes en el entrenamiento, no es de buena calidad. Al realizar el 
inpainting, se llega a lo siguiente:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 272.00px;"><img alt="" src="Informeconsolidado_files/image48.png" style="width: 602.00px; height: 272.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>La
 imagen resultante no está distorsionada, sin embargo, las 
especularidades en su mayor parte no fueron completamente corregidas. 
Esta observación concuerda con las métricas obtenidas para las 
especularidades: un recall bajo explica que no todos los píxeles de 
especularidades son detectados, mientras que la </span><span class="c13">precision</span><span class="c0">&nbsp;algo
 más alta, implica que aquellos píxeles marcados como especularidades, 
por lo general lo son, y por ello no hay grandes distorsiones. Teniendo 
en cuenta que la mayoría de los píxeles fueron usados para entrenar, los
 resultados no son tan buenos.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Para finalizar, se pone a prueba el nuevo enfoque para la imagen del otro video, llegando a lo siguiente:</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image6.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c4"><span class="c0">Es
 interesante ver que buena parte de las conclusiones extraídas en el 
caso anterior, se repiten: varias especularidades que no se volvieron a 
detectar y muy poca o casi nada de distorsión agregada &nbsp;a la 
imagen.</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span class="c0">Para
 finalizar esta etapa, se muestra la imagen original y la restaurada 
mediante los dos métodos: por umbralización (arriba) y por aprendizaje 
(debajo), para luego extraer conclusiones.</span></p><p class="c4"><span class="c0">&nbsp;</span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image52.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 261.33px;"><img alt="" src="Informeconsolidado_files/image33.png" style="width: 602.00px; height: 261.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Es
 interesante ver cómo ninguno de los dos enfoques obtiene buenos 
resultados. Cuando se va por el lado de umbralizar, en general, se 
marcan como especulaciones los puntos brillantes, típicos de 
especularidades. Esto hace que las partes azuladas no sean detectadas. 
Sin embargo, estas partes son difíciles de establecer, incluso 
haciéndolo de forma manual, y no han sido reportadas en trabajos 
previos.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Respecto
 al enfoque mediante aprendizaje, sus resultados no fueron para nada 
destructivos. Esto puede estar ligado al hecho de que no fue capaz de 
encontrar todas las especularidades. Una posibilidad para mejorar estos 
resultados, puede ser definir más cantidad de clases, en vez de tener 
solo especularidades/no especularidades. Por otro lado, se necesita 
tener más imágenes etiquetadas, con mejores criterios. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Para
 el caso de detectarlas como umbralización, quizá haya que entender un 
poco mejor qué es lo que da origen al azulado entorno a las 
especularidades, y agregar algún criterio que permita captar estos 
píxeles. Una gran debilidad que tiene este enfoque es que al aplicar los
 umbrales configurados en una imagen a otra distinta, la imagen 
resultante se ve degradada.</span></p><h1 class="c30 c32" id="h.ox7gd5nnnorb"><span class="c9"></span></h1><h1 class="c30 c32" id="h.d1afkxz973ov"><span class="c9"></span></h1><hr style="page-break-before:always;display:none;"><h1 class="c30 c32" id="h.d8mmlhfof1fa"><span class="c9"></span></h1><h1 class="c30" id="h.c9sjkd3l7zys"><span class="c9">Segmentación de instrumentos</span></h1><p class="c2"><span>Inicialmente
 interesa distinguir qué píxeles corresponden a tejido y cuáles a 
instrumentos. Para esta tarea es imperioso contar con imágenes de 
operaciones etiquetadas, tanto para su entrenamiento como para su 
evaluación. Los videos provistos no están anotados, y hacerlo demanda 
mucho esfuerzo, por lo que se trabajó con un </span><span class="c12"><a class="c10" href="http://opencas.webarchiv.kit.edu/" target="_blank">conjunto de datos público</a></span><span>&nbsp;que surge como resultado del </span><span class="c12"><a class="c10" href="https://arxiv.org/pdf/1805.02475.pdf" target="_blank">trabajo</a></span><span>&nbsp;hecho por Bodenstedt et al</span><sup><a href="#ftnt3" id="ftnt_ref3">[3]</a></sup><span class="c0">.
 para comparar distintos algoritmos de segmentación en este tipo de 
imágenes. A su vez, estas comparaciones fueron tomadas como punto de 
partida para el presente trabajo. Los autores comparan varios algoritmos
 para la segmentación y seguimiento de instrumentos. Para la 
segmentación, presenta dos grandes familias de algoritmos: los que usan 
redes neuronales convolutivas, y los que usan Random Forest sobre 
features extraídas de las imágenes. De esta comparación concluyen que 
las redes neuronales tienen mejores resultados. Sin embargo, a los 
efectos del presente proyecto se contemplan con los que utilizan Random 
Forest.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>En </span><span class="c12"><a class="c10" href="https://link.springer.com/article/10.1007%2Fs11548-015-1291-1" target="_blank">uno de esos trabajos</a></span><span class="c0">,
 dice que utiliza la implementación con soporte de GPU provista por 
OpenCV y se ejecuta en tiempo real. Toma como features múltiples 
espacios de colores como HSV, RGB, LAB, y Opponent además de información
 del gradiente. De estos, dice que se queda con los mejores luego de 
probar todas las combinaciones, aunque no aclara cuáles son ni cuántos 
son. Utiliza bosques de 100 árboles con profundidad máxima 10. Como 
tiene varias categorías de píxeles, entrena un bosque por cada una.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>El </span><span class="c12"><a class="c10" href="https://ieeexplore.ieee.org/document/6359786/" target="_blank">otro trabajo</a></span><span>,
 es entrenado en CPU, con la implementación de OpenCV. Nuevamente 
entrenan un bosque para cada tipo de instrumento. Cada bosque lo limitan
 a un máximo de 50 árboles con no más de 10 niveles de profundidad para 
cada uno. Cómo features, inicialmente plantean usar varios espacios de 
colores, módulo del gradiente y otras más estructurales, como SIFT y 
HoG. Luego de un proceso de selección de atributos reduce a Hue, 
Saturación, Opponent 2 y 3. Para seleccionar atributos tomaron dos 
criterios: la distancia de </span><span class="c12"><a class="c10" href="https://en.wikipedia.org/wiki/Bhattacharyya_distance" target="_blank">Bhattacharyya</a></span><span>&nbsp;y la </span><span class="c13">variable importance</span><span class="c0">. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Tomando
 como referencia los trabajos mencionados, se comienza por usar como 
features las de ambos trabajos: HSV, RGB, LAB, Opponent y módulo del 
gradiente.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>Para
 convertir de RGB a HSV y LAB se usaron las implementaciones provistas 
por OpenCV. Sin embargo, no existe una en python para convertir a RGB a 
Opponent, por lo que fue implementado siguiendo su </span><span class="c12"><a class="c10" href="https://github.com/opencv/opencv/blob/2.4/modules/features2d/src/descriptors.cpp" target="_blank">implementación en C</a></span><span class="c0">, que es básicamente aplicar la siguiente definición:</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 249.00px; height: 196.00px;"><img alt="" src="Informeconsolidado_files/image53.png" style="width: 249.00px; height: 196.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">El módulo del gradiente fue calculado con el operador sobel.</span></p><p class="c2 c3"><span class="c0"></span></p><h2 class="c16" id="h.xfg4imgbzw1d"><span class="c7">Selección de atributos</span></h2><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">El
 primer paso fue leer todas las imágenes, tanto de entrenamiento como de
 test, convertir cada pixel a su espacio de características y mezclarlos
 de forma aleatoria. Como la saturación va de 0 a 1, para mantener 
rangos similares a los demás, fue multiplicada por 255. Esto no forma 
parte de los trabajos previos. Luego comienza la etapa de selección de 
atributos. Esta etapa fue particularmente importante no solo por reducir
 dimensiones, sino también por reducir la memoria utilizada.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">En
 primera instancia se graficaron todos los pares de variables, 
incluyendo las features con la variable a predecir, y un histograma de 
cada una de las features. El resultado más interesante de estas 
gráficas, es que el espacio LAB mantiene una relación lineal con el HSV.
 La explicación a esto es sencilla: por un bug en el código de 
extracción de features quedó duplicada la extracción del color LAB. Si 
bien el error era muy evidente en esta gráfica inicial, fue pasado por 
alto y solo se detectó en etapas muy tardías del proyecto. En esta etapa
 puntualmente se descartó el espacio de colores LAB, dejando lo que se 
creía que era HSV, por ser más intuitivo de entender. Como consecuencia 
de esta confusión, en las gráficas en que figura HSV debería figurar 
LAB. Notar además que, por lo visto antes, al intentar reescalar el 
canal S, en realidad se reescaló el A.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 487.50px; height: 487.50px;"><img alt="" src="Informeconsolidado_files/image3.png" style="width: 569.85px; height: 570.80px; margin-left: -36.92px; margin-top: -50.17px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>Luego se procedió a ver la importancia de las variables (</span><span class="c13">variable importance)</span><span>.
 El trabajo de referencia indica que esta se calcula entrenando un 
bosque en todas las variables posibles y asignando a cada una un puntaje
 basado en la cantidad de veces que el árbol se parte por esa variable. 
En el presente trabajo, entrenamos un bosque aleatorio (</span><span class="c12 c13"><a class="c10" href="http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html" target="_blank">RandomForestClassifier</a></span><span class="c13">)</span><span>&nbsp;con 300 árboles. Sklearn provee un atributo llamado </span><span class="c13">feature_importances_</span><span>&nbsp;para
 cada árbol del bosque, que es exactamente lo que plantea el trabajo de 
referencia: la cantidad de veces que un nodo se parte para una </span><span class="c13">feature</span><span class="c0">&nbsp;dada. Estos valores se promedian y se reportan en la siguiente imagen, con su correspondiente desviación estándar:</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image27.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Pese
 a que su desviación estándar es bastante alta en todos los casos, está 
bastante claro que tanto S como Op0 son muy buenos atributos, mientras 
que el módulo del gradiente es muy malo. El siguiente punto es comparar 
la distancia de Bhattacharyya para cada uno, esta la definimos de la 
siguiente forma:</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 255.00px; height: 60.00px;"><img alt="" src="Informeconsolidado_files/image32.png" style="width: 255.00px; height: 60.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span>donde</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 26.00px; height: 26.00px;"><img alt="" src="Informeconsolidado_files/image10.png" style="width: 62.00px; height: 26.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>y</span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 21.50px; height: 26.00px;"><img alt="" src="Informeconsolidado_files/image10.png" style="width: 62.00px; height: 26.00px; margin-left: -40.50px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span>son
 las distribuciones, aproximadas por los histogramas normalizados, de 
los objetivos (tejido/instrumento) para cada una de las features </span><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 14.00px; height: 20.00px;"><img alt="" src="Informeconsolidado_files/image22.png" style="width: 14.00px; height: 20.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span><span class="c0">.
 Esta distancia mide qué tan bien separa esta característica a los 
objetivos, como una distancia entre los histogramas. Los resultados 
fueron los siguientes:</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 460.00px; height: 345.31px;"><img alt="" src="Informeconsolidado_files/image44.png" style="width: 460.00px; height: 345.31px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">Es
 interesante ver que según ambos criterios, el módulo del gradiente debe
 ser descartado. Sin embargo, la siguiente que sugiere descartar la 
distancia de Bhattacharyya, es S, que en el criterio anterior era la 
feature más relevante. Algo similar sucede con Op2: el criterio de 
distancia sugiere que es el más importante, pero con el criterio 
anterior debería ser descartado.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Como
 un tercer criterio, se implementó eliminación recursiva de 
características. Esta técnica consiste en entrenar un clasificador 
Random Forest, y luego eliminar la característica que aporta menos 
información, según el criterio que vimos inicialmente. Luego se vuelve a
 entrenar con las características que quedaron, así sucesivamente hasta 
eliminarlas a todas. Antes de comenzar este proceso, se separan los 
ejemplos existentes en dos conjuntos: uno de entrenamiento y otro de 
test, de manera que la selección de cuál característica eliminar se hace
 con el de entrenamiento, y el de test se usa para monitorear el 
desempeño del clasificador con la eliminación de características. Como 
medida del desempeño del clasificador se tomó la medida F1.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image5.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">En
 esta gráfica, de izquierda a derecha, se muestra la medida F1 sobre el 
conjunto de test. La primer columna resulta de entrenar con todas las 
características. En este entrenamiento la característica que resulta 
menos útil es el módulo del gradiente, que aparece sobre la base de la 
columna. Esta es eliminada, y se entrena con las que quedan. El 
resultado de este segundo entrenamiento figura en la segunda columna, 
junto a la característica que menos información aportó para este 
entrenamiento. Aplicando esto se llega a la última columna, en la que se
 entrenó únicamente con Op0. Es interesante ver que tras eliminar las 
primeras 6 características, el desempeño del clasificador no cambia para
 nada, e incluso al remover la séptimo cambia muy poco.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Con
 estos resultados, se eliminan de las características el módulo del 
gradiente, el espacio de color LAB, Hue, Opponent 2 y Blue. De esta 
forma se pasa de 13 características iniciales a 6.</span></p><p class="c2 c3"><span class="c0"></span></p><h2 class="c16" id="h.pd1dna3ci7ma"><span class="c7">Entrenamiento</span></h2><p class="c2"><span>El
 conjunto de entrenamiento consta de 40 imágenes de 640 x 480, esto 
totaliza 49.152.000 píxels, cada uno con un vector de características de
 tamaño 6, y su correspondiente </span><span class="c13">ground truth</span><span class="c0">. Esto implica una gran cantidad de memoria necesaria solamente para cargarlos de disco. </span></p><p class="c2"><span class="c0">Esto
 llevó a múltiples problemas durante la ejecución. Además, al parecer 
existe un bug en la implementación de sklearn, que al utilizar 
RandomForestClassifier junto con GridSearchCV, la ejecución se cuelga y 
nunca termina. Posiblemente esté vinculado con alguna condición de 
deadlock o memory leaks. Situaciones como esta han sido reportadas por 
varios usuarios, pero aún no se ha aclarado su causa.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Para
 mitigar estos problemas, en vez de entrenar con los 49 millones de 
píxeles disponibles, únicamente se utilizan la mitad de ellos, tomados 
aleatoriamente. Para ajustar los parámetros se utilizó validación 
cruzada de tamaño 2, como lo sugieren los trabajos previos. Los 
parámetros que se probaron fueron los siguientes: cantidad de árboles en
 el bosque (50 y 100) y profundidad máxima de los árboles (5 y 10). Esto
 totaliza 8 ejecuciones. Son muy pocos parámetros a probar, debido a la 
gran cantidad de ejemplos. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Los resultados son los siguientes:</span></p><p class="c4 c3"><span class="c0"></span></p><a id="t.4585fce44e9c38ece8b2fe540fa0c089c7db11e0"></a><a id="t.0"></a><table class="c38"><tbody><tr class="c5"><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">Profundidad máxima</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">Cantidad de estimadores</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">Medida F</span></p></td></tr><tr class="c5"><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">5</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">50</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">0.625 (+/-0.002)</span></p></td></tr><tr class="c5"><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">5</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">100</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">0.628 (+/-0.000)</span></p></td></tr><tr class="c5"><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">10</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">50</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">0.643 (+/-0.000)</span></p></td></tr><tr class="c5"><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">10</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">100</span></p></td><td class="c8" colspan="1" rowspan="1"><p class="c15"><span class="c0">0.625 (+/-0.001)</span></p></td></tr></tbody></table><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Lo
 reportado es el promedio de la medida F más menos dos veces la 
desviación estándar durante la validación cruzada. No fue un gran 
resultado, pero debido al tiempo que llevo su ejecución, se siguió 
adelante con el test. </span></p><p class="c4 c3"><span class="c0"></span></p><h2 class="c24" id="h.y5k2n7ewozzm"><span class="c7">Testing</span></h2><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Para
 esto, se entrenó un clasificador con los parámetros encontrados. y 
luego se lo evaluó con el conjunto de test, que consiste de 18.432.000 
píxeles. El resultado fue el siguiente:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span class="c18">&nbsp;
 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; 
precision &nbsp; &nbsp;recall &nbsp; &nbsp; f1-score &nbsp; &nbsp; 
&nbsp; support</span><span class="c0"><br><br> &nbsp; &nbsp; &nbsp; 
Tissue &nbsp; &nbsp; &nbsp; &nbsp;0.96 &nbsp; &nbsp; &nbsp; &nbsp; 
&nbsp; 0.97 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.96 &nbsp; &nbsp; &nbsp; 
&nbsp;16704432<br> Instrument &nbsp; &nbsp; &nbsp; 0.64 &nbsp; &nbsp; 
&nbsp; &nbsp; &nbsp; 0.57 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.60 &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp;1727568<br><br> &nbsp; &nbsp;avg / total 
&nbsp; &nbsp; &nbsp; 0.93 &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;0.93 &nbsp; 
&nbsp; &nbsp; &nbsp; &nbsp;0.93 &nbsp; &nbsp; &nbsp; &nbsp; 18432000<br></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Puede
 notarse que los valores de F-score para los instrumentos, que es la 
medida que se estaba reportando, disminuye un 5%. Esta disminución es 
esperable, ya que, si bien durante la validación cruzada ambas 
ejecuciones habían dado lo mismo, la lista de ejemplos de entrenamiento 
que se usó para la validación cruzada, los píxeles que van a parar a 
cada una de las dos particiones son prácticamente los mismos por venir 
de las mismas imágenes. Sin embargo, ahora estamos entrenando con 
píxeles de unas imágenes y probandolo con píxeles de otras diferentes, 
el problema se hace un tanto más difícil. Quizá lo mejor hubiese sido 
agregar al pipeline la etapa de extracción y selección de features, y de
 esta manera asegurar que durante la validación cruzada, todas las 
particiones tuvieran píxeles de diferentes imágenes.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Además
 se reporta el área bajo la curva ROC y PR. La curva ROC muestra cómo 
cambia la tasa de verdaderos positivos y falsos positivos en la medida 
que se mueve un umbral sobre las probabilidades que predice el modelo 
para el conjunto de test. Luego se mide el área bajo la curva, e 
idealmente debe acercarse a uno. En un caso aleatorio vale 0.5 y es la 
referencia que figura punteada en rojo. Esta curva nos muestra el poder 
de discriminación del clasificador.</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image13.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">La
 curva ROC considera el desempeño en ambas clases, y es por eso en parte
 que da un valor alto. El clasificador entrenado es bueno detectando 
tejido pero no así con instrumentos. Esto ya era evidente con las 
primeras métricas observadas: pese a que tiene mal desempeño 
clasificando instrumentos, como estos son sólo el 10%, al promediar 
ambas tareas ponderadas por la cantidad de píxeles evaluados en cada 
clase, las métricas siguen siendo buenas. Para aislar este efecto, se 
reporta también la curva PR. Esta es similar a la ROC, pero en ella se 
muestra como cambian la precision y recall al variar un umbral sobre las
 probabilidades.</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image31.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c2"><span class="c0">En
 este caso, el área bajo la curva es bastante menor, e inclusive en un 
tramo está por debajo de la diagonal. Esta métrica es más objetiva que 
la anterior ya que aísla el efecto del desbalance entre clases. </span></p><p class="c4 c3"><span class="c0"></span></p><h2 class="c24" id="h.r3ts95vpgma4"><span class="c7">Análisis cualitativo</span></h2><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Por último, se implementó un script que permite aplicar el clasificador entrenado, tanto a imágenes como a vídeo. </span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Para
 una imagen, se pueden obtener tres resultados: la imagen coloreada en 
rojo para el tejido y azul para instrumento, la máscara aprendida, o un 
mapa de calor con las probabilidades de que cada pixel sea un 
instrumento. Vemos a continuación un ejemplo de cada una de ellas, sobre
 el conjunto de imágenes de entrenamiento. </span></p><p class="c4 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image34.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Imagen original</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image47.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Máscara aprendida</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image29.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Imagen coloreada</span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image14.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c0">Mapa de calor</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Lo
 primero a notar es que pese a que esta imagen es parte del conjunto de 
entrenamiento, no todos sus píxeles tienen por qué haber estado durante 
el entrenamiento, ya que la mitad de todos los píxeles de entrenamiento 
fueron ignorados de forma aleatoria.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Otro
 punto a notar, y donde podría haber una mejora importante, es que los 
píxeles detectados no son continuos: hay varios píxeles de instrumentos 
sueltos. Esto se podría mejorar a posteriori, es decir, con la máscara 
aprendida forzar que sean continuos o incluir de alguna manera esta 
información en el modelo o espacio de características consideradas.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">En
 la esquina superior derecha detecta instrumento cuando en realidad no 
lo hay. Si se mira de cerca esa parte de la imagen, puede notarse que 
esto en realidad es parte del tubo por donde va la cámara, sin embargo 
estos píxeles en el conjunto de entrenamiento aparecen marcados como no 
instrumento. Algo similar ocurre con el pequeño reborde negro que tiene 
la imagen, y esto se repite más acentuadamente, en las imágenes de 
entrenamiento de la operación 4. En ellas, existe un borde negro 
bastante notorio:</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image26.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Imagen de entrenamiento</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image4.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Máscara correspondiente.</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Esto
 no necesariamente explica el mal desempeño del clasificador, aunque 
dificulta la tarea, ya que si bien estos píxeles no son parte de 
instrumento, tampoco lo son de tejido. Al ser completamente negros, y 
tener características únicamente basadas en color, es más probable 
tomarlo como parte de instrumento que de tejido. Una solución posible es
 ignorar estos píxeles o agregarlos a una tercera clase.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Veamos a continuación un ejemplo del conjunto de testing</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image24.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Imagen original</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image23.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Máscara aprendida</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image37.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Imagen coloreada</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image16.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c0">Mapa de calor</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Esta
 imagen tiene una particularidad que no se dio en la anterior. Abajo a 
&nbsp;la izquierda hay algunas especularidades que el clasificador 
etiquetó como instrumento. Esto no tiene que ver necesariamente con que 
sea una imagen de testing. Sin embargo, otra gran parte de 
especularidades fueron clasificadas correctamente como tejido.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Por
 último se muestra un frame obtenido partir de uno de los videos 
provistos por el Hospital de Clínicas. La idea es ver qué tanto se 
extrapolan los resultados aprendidos, aunque sea una primera 
aproximación cualitativa.</span></p><p class="c4 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 481.33px;"><img alt="" src="Informeconsolidado_files/image40.png" style="width: 602.00px; height: 481.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Imagen original</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 481.33px;"><img alt="" src="Informeconsolidado_files/image7.png" style="width: 602.00px; height: 481.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Máscara aprendida</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 481.33px;"><img alt="" src="Informeconsolidado_files/image21.png" style="width: 602.00px; height: 481.33px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span class="c0">Imagen segmentada</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6 c3"><span class="c0"></span></p><p class="c6"><span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 602.00px; height: 452.00px;"><img alt="" src="Informeconsolidado_files/image19.png" style="width: 602.00px; height: 452.00px; margin-left: 0.00px; margin-top: 0.00px; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px);" title=""></span></p><p class="c6"><span class="c0">Mapa de calor</span></p><p class="c6 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Los
 resultados no son muy buenos. Los píxeles de instrumentos detectados 
están más dispersos que en los casos anteriores, ya que confunde más a 
menudo las especularidades con instrumentos. Así mismo, no es capaz de 
detectar toda la pinza.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Como complemento a este trabajo, se presenta el video mencionado clasificado con los criterios aprendidos.</span></p><p class="c2 c3"><span class="c0"></span></p>

<p class="c6">

<span style="overflow: hidden; display: inline-block; margin: 0.00px 0.00px; border: 0.00px solid #000000; transform: rotate(0.00rad) translateZ(0px); -webkit-transform: rotate(0.00rad) translateZ(0px); width: 225.00px; height: 225.00px;">

<video controls="controls" width="225" height="225">
  <source src="Informeconsolidado_files/out.mp4" type="video/mp4">
</video>

</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2 c3"><span class="c0"></span></p><h1 class="c30" id="h.u93bpobo0wn"><span class="c9">Conclusiones y trabajo a futuro</span></h1><p class="c4 c3"><span class="c0"></span></p><p class="c2"><span class="c0">La
 segmentación de instrumentos a partir de imágenes no es para nada 
trivial. La solución encontrada tiene niveles muy malos de performance. 
Como trabajo a futuro queda repetir los experimentos añadiendo 
correctamente el espacio HSV a las features utilizadas y volver a 
ejecutar los pasos descritos. Eso puede que mejore en algo la 
performance. Además, parece razonable dado los tiempos de cómputo 
requeridos, pasar a una implementación de RandomForest en GPU. Si bien 
no hay una en sklearn, si existe una en OpenCV. Otro punto que llama la 
atención es la cantidad de memoria que se necesita para ejecutar 
correctamente. Este punto hace pensar que quizá sea necesario reducir 
aún más las dimensiones. Una posibilidad es eliminar más features, 
aunque no hay que descartar utilizar algo como PCA.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">Queda
 pendiente evaluar el papel que juegan las especularidades con la 
segmentación actual, y como complemento, ver el efecto que causa 
entrenar y clasificar sobre imágenes previamente corregidas.</span></p><p class="c2 c3"><span class="c0"></span></p><p class="c2"><span class="c0">En
 el marco de lo exploratorio del trabajo, en un video se ve una grapa 
que cae, al parecer de forma accidental. Hay que entender con los 
expertos en la materia si es correcto que esto ocurra, ya que es un 
evento potencialmente peligroso, del que podrían ser alertados. Otro 
ejemplo, que no aparece en los videos pero si en las imágenes de 
ejemplo, es el algodón. Quizá estos cuerpos extraños que tienen más 
probabilidad de quedar olvidados allí deberían ser reconocidos en otra 
categoría.</span></p><p class="c4 c3"><span class="c0"></span></p><hr class="c40"><div><p class="c15 c21"><a href="#ftnt_ref1" id="ftnt1">[1]</a><span class="c31">Automatic Detection of Specular Reflections in Uterine Cervix Images, 2006</span></p></div><div><p class="c15 c21"><a href="#ftnt_ref2" id="ftnt2">[2]</a><span class="c31">&nbsp;Telea,
 Alexandru. "An image inpainting technique based on the fast marching 
method." Journal of graphics tools 9.1 (2004): 23-34</span></p></div><div><p class="c15 c21"><a href="#ftnt_ref3" id="ftnt3">[3]</a><span class="c31">&nbsp;Comparative evaluation of instrument segmentation and tracking methods in minimally invasive surgery, 7 May 2018.</span></p></div></body></html>